\documentclass[a4paper]{usiinfbachelorproject}

\captionsetup{labelfont={bf}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%% PACKAGES %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{float}
\usepackage{amsmath}
\usepackage{todonotes}
% \usepackage[disable]{todonotes} % Disables all TODOs
\usepackage{tikz}
\usetikzlibrary{arrows.meta, positioning}

\usepackage{titlesec}
\titlespacing*{\section}{0pt}{1.5ex plus 1ex minus .2ex}{1ex plus .2ex}
%%% Main Body %%%

\author{Davide Frova}

\title{\textbf{Exploring the Learning by Teaching Paradigm with Social Robots}}
\subtitle{Exploratory Studies on Learning by Teaching with Social Robots using Wizard-of-Oz Control}
\versiondate{\today}

\begin{committee}
%With more than 1 advisor an error is raised...: only 1 advisor is allowed!
\advisor[Istituto Dalle Molle di Studi sull\'Intelligenza Artificiale, IDSIA, Switzerland]{Prof.}{Monica}{Landoni}
%You can comment out  these lines if you don't have any assistant
\coadvisor[Istituto Dalle Molle di Studi sull\'Intelligenza Artificiale, IDSIA, Switzerland]{ }{Antonio}{Paolillo}

\end{committee}

\abstract {
Social robots are becoming increasingly prevalent in educational settings, offering new opportunities for collaborative learning and social skill development. 
This project focuses on the design of tools to support the exploration of the Learning by Teaching (LbT) paradigm, where children interact with a social robot to foster turn-taking and prosocial behavior. 
As part of the TESORO project, we designed and evaluated a web-based dashboard that enables Wizard-of-Oz control of a RoboMaster EP robot in a Lego-building task with children. 
The robot intervenes to regulate turn-taking and is perceived as a peer-like entity rather than an authority figure. 
We conducted three formative, iterative experiments involving HCI experts, high school students, and middle school children, leading to refinements in the robot's behavior and the dashboard interface to be used in future such studies. 
Our findings highlight the importance of multimodal feedback, personalization, and emotional awareness in child-robot interaction. 
The study provides a foundation for future semi-autonomous robot behavior and real-time learning from children's regulatory input.

\textbf{Keywords}: Social robots; Learning by Teaching; Human-Robot Interaction; Wizard-of-Oz; Formative Evaluation; Child-Computer Interaction.
}

\begin{document}
\maketitle
\tableofcontents\newpage
%\listoffigures\newpage

\section{\textbf{Introduction}}

How can we design social robots that help children develop prosocial skills, like turn-taking, while also learning from the children themselves?
This project explores this question through the lens of the Learning by Teaching (LbT) paradigm, an approach where learners consolidate their understanding by teaching others.
This approach has been studied in both educational software and robotics contexts~\cite{biswas2005learning, 10.5898/JHRI.1.1.Tanaka}.
In this context, the robot is not merely a passive tool but a teachable partner embedded in a collaborative play scenario.

This project is embedded within the broader TESORO initiative, a research effort aiming to co-design accessible social robots that foster mutual learning experiences with children, including those with intellectual disabilities~\cite{landoni2025tesoro}.
While TESORO's long-term ambition is to develop adaptive and emotionally intelligent robots, this bachelor thesis contributes to its early-stage development by focusing on a Wizard-of-Oz setup: a human-controlled robot simulates behavior to test early interaction concepts and gather formative feedback.
The effectiveness and practicality of the Wizard-of-Oz (WoZ) approach in HRI research have been well established in the literature~\cite{weiss2010userWOZ, rietz2021woz4uPepper, SCHOONDERWOERD2022102831}.

While existing tools such as Foxglove~\cite{foxglove} or RViz~\cite{rviz2} offer powerful robotic control and visualization capabilities, they are designed primarily for technical debugging and telemetry inspection.
These platforms are not suited to educational or social interaction contexts, where real-time adaptability, intuitive use, and expressiveness are key.
In contrast, the dashboard presented in this work was specifically co-designed for playful and socially attuned interaction with children.
It prioritizes ease of use for non-technical operators (e.g., educators or researchers) and enables the delivery of nuanced, multimodal feedback during Learning by Teaching (LbT) scenarios.
These limitations are especially apparent in scenarios involving young children, where co-design and accessibility play a critical role~\cite{osti_10386132, rose2019participatory}.

This thesis presents a dual contribution: the development of a web-based dashboard for Wizard-of-Oz control of a social robot, and an exploratory initial investigation into how children perceive, interact with, and teach the robot in collaborative play scenarios.
These two strands evolved in parallel and informed each other throughout the project.
The dashboard's design was refined iteratively through hands-on formative evaluation sessions, while these same sessions served as a space to initially explore children's social expectations, feedback preferences, and engagement strategies.
This methodology reflects a formative evaluation approach commonly used in HRI to iteratively adapt both behavior and interaction design~\cite{love2024teachable}.
Through three walkthrough experiments, we analyzed how multimodal feedback such as lights, movement, and sound was interpreted and how the robot's perceived role evolved from an authoritative presence to a playful peer.
These insights not only shaped the technical design but also informed the behavioral framing of the robot, highlighting the mutual reinforcement between interface usability and socially attuned robot conduct.

Rather than testing specific learning outcomes, this work aims to inform the co-design of child-robot interaction tools.
It demonstrates how formative evaluation can guide both interface design and behavioral strategies in child-facing social robots.

While the system remains operator-driven, it lays the foundation for future research into semi-autonomous behavior grounded in children's regulatory input.

\subsection{\textbf{Report Structure}}
The rest of the report is organized as follows: Section~\ref{sec:background} presents the related work and theoretical framework; Section~\ref{sec:design} outlines the experiment scenario and research goals; Section~\ref{sec:system} describes the dashboard's architecture and implementation; Section~\ref{sec:evaluation} presents the evaluation process and findings across three iterative walkthroughs, highlighting how user feedback informed the co-design of the final system; and Section~\ref{sec:conclusions} concludes with future directions.

\section{\textbf{State of the Art}}\label{sec:background}
The integration of social robots in educational contexts has evolved significantly over the past two decades, moving from structured teaching tools to more dynamic, participatory agents capable of engaging in mutual learning with children.
One promising paradigm in this space is \textit{Learning by Teaching (LbT)}, which posits that learners consolidate their own understanding by teaching others~\cite{biswas2005learning}.
In Human-Robot Interaction (HRI), LbT has proven effective at increasing both engagement and educational outcomes~\cite{10.5898/JHRI.1.1.Tanaka, love2024teachable}.

Multiple systems have explored teachable robots in child-centered scenarios.
Tanaka and Matsuzoe~\cite{10.5898/JHRI.1.1.Tanaka} showed how children teaching a NAO~\cite{nao_robot} robot led to improved vocabulary learning, while Love et al.~\cite{love2023adapting, love2024teachable} developed reinforcement learning mechanisms to adapt dialog and sustain engagement.
These systems highlight the benefits of positioning the robot as a learner rather than a teacher, shifting power dynamics and empowering the child.

While promising, these approaches often begin with exploratory setups grounded in the Wizard-of-Oz (WoZ) methodology, where a human simulates robot behavior to evaluate interaction strategies before full autonomy is developed~\cite{weiss2010userWOZ, rietz2021woz4uPepper, SCHOONDERWOERD2022102831}.
Tools like WoZ4U~\cite{rietz2021woz4uPepper} are effective for humanoid robots like Pepper~\cite{pepper_robot} but are tightly coupled to specific hardware and are not optimized for more mobile, non-humanoid platforms like the RoboMaster EP.
This highlights a technical gap in flexible, child-facing WoZ systems that support socially attuned feedback and intuitive control.

Moreover, existing robotic middleware tools such as Foxglove~\cite{foxglove} and RViz2~\cite{rviz2} are tailored for debugging and telemetry, not for dynamic, socially expressive feedback.
They require technical expertise and offer limited support for multimodal, context-aware interaction, especially with children.
Our work addresses this shortcoming through the development of a web-based dashboard designed explicitly for expressive, real-time robot control in educational settings.
The dashboard was iteratively co-designed with researchers and children to meet the demands of playful and socially responsive interactions.

This contribution aligns with the goals of the broader \textit{TESORO} project~\cite{landoni2025tesoro}, which envisions accessible, emotionally intelligent robots that foster mutual learning, especially for children with intellectual disabilities.
TESORO promotes the idea that social robots should not only guide behavior but also learn from the child, turning the robot into a reflective tool for self-regulation and emotional awareness.

Co-design and participatory methods have gained traction in child-robot interaction research, affirming that children are more engaged when they can personalize or influence a robot's behavior~\cite{osti_10386132, rose2019participatory}.
Such methods empower children and ensure that social robot designs are inclusive and developmentally appropriate.

Additionally, the study by Arreghini et al.~\cite{arreghini2022exploring} explores how humans perceive and prefer different robot collaboration strategies (passive, reactive, proactive) in a cognitively and physically demanding task.
While not focused on children, the findings underline that proactive behaviors, especially when paired with multimodal feedback (e.g., lights, sounds, actions), lead to more positive user evaluations, even when task completion takes longer.
These insights are highly relevant to child-robot interaction design, where responsiveness and perceived helpfulness also shape engagement and acceptance.

Our work contributes to this growing body of research by integrating:
(1) an expressive and adaptable control interface for WoZ experimentation,
(2) a structured yet flexible study scenario centered on Learning by Teaching, and
(3) an exploratory methodology that surfaces how children engage with, perceive, and guide robot behavior.

Crucially, the development of the dashboard and the exploration of social interaction dynamics were pursued in tandem.
This dual approach reflects the project's broader aim, not just to implement an interface, but to understand how such an interface can scaffold meaningful, teachable moments and social learning experiences.
It lays essential groundwork for future investigations into semi-autonomous systems capable of adapting to children's real-time regulatory input.

\section{\textbf{Experiment Design and Goals}}\label{sec:design}

The experiment designed for this project aims to explore how a social robot can support children's learning through turn-taking regulation.
It is part of the Usage Research necessary to drive the design of effective child-robot interactions that support the Learning by Teaching (LbT) paradigm.

\subsection*{\textbf{Motivation and Context}}
The experiment was designed to investigate how children can teach a robot socially appropriate behaviors in a collaborative, rule-based play scenario.
Within the TESORO project, this experiment serves as a preliminary study to simulate the LbT dynamic, with the robot acting as both a participant and a teachable agent.

\subsection*{\textbf{Research Objectives}}
The primary goals of the experiment are:
\begin{itemize}
    \item To explore how a social robot can foster turn-taking and collaboration between children.
    \item To evaluate how children respond to multimodal robot feedback (e.g., lights, movement, sound).
    \item To study how children engage in teaching the robot correct behaviors, especially in social contexts.
    \item To assess the feasibility and expressiveness of Wizard-of-Oz robot control during child-robot interactions.
\end{itemize}

\subsection*{\textbf{Scenario and Task Design}}
The chosen task, collaboratively building a Lego tower, requires communication, turn-taking, and shared goals.
This makes it an ideal setting for observing how robot feedback and teaching dynamics unfold naturally.

\subsection*{\textbf{Three-Step Experiment Structure}}
The experiment was structured in three progressive phases, each enabling the team to explore different facets of LbT:

\begin{itemize}
    \item \textbf{Step 1: Robot as Regulator}: Two children collaboratively build a Lego tower while the robot observes and intervenes in cases of turn-taking violations or disruptive behavior using feedback such as LEDs, movement, or sounds.
    \item \textbf{Step 2: Child as Regulator}: One child mediates the interaction between the robot and a peer, guiding the robot's actions and providing corrections. This directly instantiates the LbT paradigm.
    \item \textbf{Step 3: Robot Learns How and When to Intervene}: The robot enacts behavior patterns influenced by previous interactions. While still controlled via Wizard-of-Oz, this phase simulates a transition toward autonomy in future implementations.
\end{itemize}

\subsection*{\textbf{Wizard-of-Oz Interface Role}}
A key component of the experiment is the web-based dashboard used to remotely control the robot in real time.
The interface was iteratively developed to support both reactive and expressive robot behaviors based on child feedback.

\subsection*{\textbf{TESORO Alignment}}
This experiment aligns with the TESORO project's longer-term vision of developing socially intelligent robots that can learn from children in real-time.
The insights gathered here lay the groundwork for future autonomous learning systems that better reflect children's expectations and communication styles.

\section{\textbf{System Design and Implementation}}\label{sec:system}

\subsection{\textbf{System Architecture}}
\subsection*{\textbf{Overview}}
The system architecture was designed to enable an operator to control a social robot in a Wizard-of-Oz fashion during experiments with children.
The main components include a web-based dashboard for real-time control~\cite{frovaaa2025hogwarts}, a communication layer based on ROS2 (Robot Operating System 2)~\cite{ros2, frovaaa2025robomaster, frovaaa2025robomasterhri}, and the RoboMaster EP robot~\cite{djirobomasterep}.
The dashboard provides intuitive controls for triggering robot interventions and monitoring status, while the ROS2 integration ensures reliable communication and execution of commands.
This architecture prioritizes usability, low latency, and safety, supporting the experimental requirements for responsive and effective interactions.
The system was designed to facilitate iterative development and easy modifications, supporting a co-design approach as outlined in the TESORO project proposal~\cite{landoni2025tesoro}.
This flexibility enables rapid adaptation of the dashboard based on feedback from stakeholders and participants during the study.

\subsection{\textbf{Component Description}}
\subsubsection*{\textbf{Dashboard}}
The dashboard is a web-based interface developed using Next.js, with the user interface built on Material UI (MUI) components for a modern and consistent look.
It allows the operator to monitor the robot's status and trigger interventions in real time. Communication with the robot is achieved via \texttt{roslibjs}~\cite{roslibjs}, which enables the dashboard to send and receive ROS messages over the network.

\subsubsection*{\textbf{Middleware Server}}
Due to a known limitation in \texttt{roslibjs} when interacting with ROS2 action servers, a minimal middleware server was implemented within the dashboard system.
This server is built with Express and uses \texttt{rclnodejs}~\cite{rclnodejs} to interface directly with ROS2, acting as a bridge between the dashboard and ROS2 action servers.
This ensures reliable execution of complex robot actions and exposes additional functionalities to the web interface.

\subsubsection*{\textbf{ROS Communication Layer}}
The communication layer is based on ROS2 and uses \texttt{rosbridge\_server}~\cite{rosbridge} running on the Ubuntu machine that controls the robot ROS2 drivers.
This layer handles the communication between the dashboard and the robot, allowing for real-time updates and command execution.

\subsubsection*{\textbf{Robot Control and Action Servers}}
The Ubuntu machine runs several ROS2 nodes and action servers, including:
\begin{itemize}
    \item \texttt{robomaster\_ros}~\cite{frovaaa2025robomaster}: ROS2 drivers for the RoboMaster EP robot, providing low-level control and sensor access.
    \item \texttt{robomaster\_hri}~\cite{frovaaa2025robomasterhri}: Custom packages for high-level robot control and human-robot interaction.
    \item Security and safety action servers: Allow the operator to immediately halt robot actions or trigger safety protocols as needed.
\end{itemize}

\subsubsection*{\textbf{RoboMaster EP Robot}}
The RoboMaster EP robot~\cite{djirobomasterep} is the physical platform used for the experiment.
It connects to the Ubuntu machine via its dedicated access point, ensuring a stable and isolated communication channel.
The robot executes commands received from the ROS2 action servers and ROS2 nodes, and provides feedback to the dashboard.


\subsubsection*{\textbf{Network and Deployment}}
All components are deployed on a local network to ensure low latency and reliability.
The dashboard can be accessed from any device on the network, while the Ubuntu machine acts as the central hub for ROS2 communication and robot control.
Since the RoboMaster EP robot connects wirelessly and provides odometry data, the system is not restricted to a laboratory setting.
This flexibility allows experiments and interactions with children to take place in more neutral or familiar environments, which can be beneficial for naturalistic studies and participant comfort.

\subsection{\textbf{Safety Considerations}}
Ensuring the safety of participants and equipment was a primary concern throughout the system design.
To address this, a dedicated ROS2 node was developed to act as an emergency stop mechanism.
This node can immediately halt all ongoing robot actions and stop the RoboMaster EP's motors.

A prominent "Emergency Stop" button is integrated into the dashboard interface, allowing the operator to trigger this safety mechanism at any time during an experiment.
When activated, the command is sent via the ROS2 communication layer to the safety node running on the Ubuntu machine, ensuring a rapid response.
This feature is essential for experiments involving children, as it provides the operator with immediate control to prevent unintended or unsafe robot behavior.
The system was tested to ensure that the emergency stop reliably overrides all other commands and brings the robot to a safe state.

\subsection{\textbf{System Architecture Diagram}}

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[
            node distance=2.5cm and 2.5cm,
            every node/.style={font=\small, align=center},
            box/.style={draw, rounded corners, minimum width=2.8cm, minimum height=1.2cm, fill=gray!10},
            arrow/.style={-Latex, thick}
        ]

        % Nodes
        \node[box] (dashboard) {Web Dashboard\\ (Next.js + roslibjs)};
        \node[box, below=of dashboard] (middleware) {Middleware Server\\ (Express + rclnodejs)};
        \node[box, below=of middleware] (rosbridge) {ROS Communication Layer\\ (rosbridge\_server)};
        \node[box, below=of rosbridge] (ubuntu) {Ubuntu Machine\\ (ROS2 Nodes \&\\ Action Servers)};
        \node[box, below=of ubuntu] (robot) {RoboMaster EP\\ Robot};

        % Connections
        \draw[arrow] (dashboard) -- node[right] {REST (POST)} (middleware);
        \draw[arrow] (middleware) -- node[right] {ROS2 Actions} (rosbridge);
        \draw[arrow] (rosbridge) -- node[right] {ROS2 Messages} (ubuntu);
        \draw[arrow] (ubuntu) -- node[right] {Wi-Fi / Access Point} (robot);

        % Direct connection from dashboard to ubuntu (ROS2 Nodes)
        \draw[arrow, dashed, blue] (dashboard.south east) to[out=-10,in=30] node[right, xshift=2mm, yshift=-2mm, text=blue] {Direct WebSocket to\\ROS2 Nodes} (rosbridge.north east);

        % Emergency stop
        \node[box, left=4.5cm of rosbridge] (safety) {Safety Node\\ (Emergency Stop)};
        \draw[arrow, thick, red] (dashboard) -- node[above left, xshift=-2mm, yshift=2mm, text=red] {Emergency Stop} (safety);
        \draw[arrow, thick, red] (safety) -- (rosbridge);

    \end{tikzpicture}
    \caption{System architecture for Wizard-of-Oz control of the RoboMaster EP robot. The dashboard communicates with the middleware server via REST (POST request), and can also communicate directly with the ROS2 nodes using WebSocket (roslibjs).}
    \label{fig:system-architecture}
\end{figure}

\subsection{\textbf{Dashboard Interface}}

The dashboard interface is the primary tool used to operate the robot during experiments.
Its evolution was directly informed by iterative feedback from all three walkthroughs and was crucial in enabling flexible and socially responsive interventions.
The dashboard was developed using Next.js with Material UI (MUI) components, and it integrates with ROS2 through a combination of \texttt{roslibjs} and a custom middleware using \texttt{rclnodejs}.

The final version of the interface (see Figure~\ref{fig:dashboard-v3-desc}) is structured into clear functional zones to streamline operator interaction and support rapid decision-making.
Key components include:

\begin{itemize}
    \item \textbf{Movement Controls:} A joystick widget allows precise navigation of the robot in real time, particularly useful when children move unpredictably or outside predefined zones.
    \item \textbf{Gripper and Arm Control:} Operators can open and close the robot's gripper independently, as well as trigger macro-actions involving arm movement, such as presenting or retrieving an object.
    \item \textbf{LED Feedback Panel:} This section enables configuration of the robot's LED lights, including color selection, blink patterns, speed, and duration. These were expanded to support both expressive feedback and task-specific signaling (e.g., requesting a particular brick color).
    \item \textbf{Sound Library:} A curated set of audio clips can be played to reinforce positive feedback. As a design choice, sounds were excluded from negative feedback to prevent distraction.
    \item \textbf{Feedback Triggers:} Predefined buttons enable quick activation of positive or negative robot responses, each available in multiple intensity levels. This feature supports nuanced behavioral interventions in response to subtle child actions.
    \item \textbf{Macro-Scenarios:} A set of programmable compound actions allows operators to trigger complex sequences (e.g., move forward + light + sound) with a single button, improving responsiveness in high-tempo interactions.
    \item \textbf{Emergency Stop Button:} A large, visually prominent button that immediately halts any ongoing robot action and stops all motors to ensure participant safety.\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{figures/dashboard_v3.png}
    \caption{Final version of the dashboard interface. All main functionalities are organized into dedicated panels, supporting expressive and safe robot operation during social interaction experiments.}
    \label{fig:dashboard-v3-desc}
\end{figure}

Throughout development, the interface was refined to enhance usability under time pressure, ensure smooth robot operation, and support playful, peer-like interactions as encouraged by co-design feedback.
The result is a robust and extensible control panel capable of supporting both current Wizard-of-Oz studies and future semi-autonomous scenarios.

\section{\textbf{Usage Research: Formative Evaluation and Iterative Co-Design}}\label{sec:evaluation}
Formative evaluation is a method of assessing a system during its development, with the goal of identifying areas for improvement and guiding the design process from its earliest stages.
In this project, formative evaluation was used as a continuous, participatory method to shape both the interface and the robot's behavioral patterns.
By integrating insights from diverse stakeholders: experts, high school students, and middle school children, the system evolved through iterative feedback.
This approach enabled responsive design decisions and supported the co-creation of meaningful child-robot interaction scenarios.

This section presents the evaluation of the system through three exploratory sessions, each structured as an iterative walkthrough.
These sessions aimed to assess the usability and flexibility of the dashboard, understand the dynamics of child-robot interaction, and collect feedback to inform the design of both the interface and the robot's behavior.
Each evaluation contributed to the co-design process by revealing user needs and contextual challenges in real time.

The sessions included:
(1) a walkthrough with HCI and accessibility experts, who role-played as elementary or middle school students (see Section~\ref{sec:expert-walkthrough});
(2) a session with high school students working in pairs and simulating collaborative behavior on the shared floor mat (see Section~\ref{sec:high-school-walkthrough}); and
(3) a pilot study with middle school children, which evolved into a semi-structured discussion and informal walkthrough focusing on robot behavior and social interaction patterns (see Section~\ref{sec:middle-school-walkthrough}).

The following subsections describe the setup, key observations, and design implications of each session, followed by a summary of the iterative changes implemented in response to user feedback.


\subsection{\textbf{Walkthrough with Experts}}\label{sec:expert-walkthrough}
\subsection*{\textbf{Setup and procedure}}
The first evaluation session was conducted in an office setting and involved two PhD researchers with expertise in Human-Computer Interaction (HCI) and accessibility.
They were asked to role-play as two children collaborating on a Lego tower building task.
Initially, they were seated on the floor approximately one meter apart, with the RoboMaster EP robot placed slightly in front of them at the midpoint of this distance, not directly between them.

The system was operated by an evaluator seated nearby at a desk, with a direct line of sight to the play zone.
A third researcher took notes throughout the session, while two additional observers with expertise in educational technology and human-robot interaction silently monitored the interaction and its outcomes.

Although the scenario was designed around pre-defined robot positions, the participants quickly moved from their initial spots, rendering the static positions ineffective.
However, the dashboard's joystick feature enabled the operator to adapt in real time by manually repositioning the robot and triggering actions.
The session unfolded as a dynamic and playful interaction between the two participants and the robot, including episodes of joint play, interaction breakdowns, and regulatory robot interventions.

After the active session, all participants engaged in an open-ended discussion, which yielded critical reflections on the interface's design, the robot's perceived role, and the effectiveness of various robot behaviors.
These observations and insights directly informed the next iteration of the dashboard.

\subsection*{\textbf{Key observations}}
\begin{itemize}
    \item The pre-defined movement positions were quickly invalidated when the "children" moved from their expected spots. The experts stressed the importance of allowing the operator to improvise and react dynamically.
    \item Negative robot feedback, such as "shaking the head" by spinning on the spot, was perceived as entertaining, rather than corrective. This could encourage children to misbehave just to see the robot react.
    \item Experts observed that opening the box with the gripper and presenting it to a participant invited them to place something inside. This could be used as a form of collaborative engagement or de-escalation.
    \item Experts emphasized the importance of distinguishing between different levels of feedback, both positive and negative.
          For positive reinforcement, a scalable approach could include combinations of lights, sounds, and movement, ranging from a simple LED blink to a celebratory motion sequence.
          Negative feedback, on the other hand, should be kept minimal and neutral: limited to red lights, with no sound, and little to no movement, possibly just a slight backward motion to signal withdrawal.
          Sound should be excluded from negative interventions entirely to prevent the robot from becoming a source of entertainment.
    \item The robot was perceived more as a "peer" than an authority figure, prompting the need to frame it accordingly in both behavior and design.
    \item Experts advised against focusing the robot's attention too much on one child, as this could lead to perceptions of unfairness.
    \item Introducing the robot to children before the task might help reduce distractions during the experiment, as curiosity would be partially satisfied beforehand.
    \item Large Lego Duplo bricks limited complexity of the task; smaller pieces might offer a more engaging challenge.
\end{itemize}

\subsection*{\textbf{Design implications}}
The insights from this session led to a clear shift in the robot's perceived role: its behavior was intentionally designed to be cooperative, inclusive, and friendly, reinforcing its identity as a peer rather than a disciplinarian.
Feedback modalities were explicitly separated into positive and negative categories to avoid ambiguity, with sound reserved only for positive reinforcement.
The dashboard was enhanced to grant the operator broader and more flexible control over the robot, including nuanced gripper manipulation and advanced LED configurations such as customizable colors, blink timing, and speed.
The importance of joystick-based improvisation was validated, ensuring the operator could dynamically adapt to unpredictable child behavior.
Overall, these refinements improved both the responsiveness and expressiveness of the system, aligning robot actions more closely with user expectations and the playful, peer-like framing advocated by the experts.

\subsection{\textbf{Walkthrough with High School Students}}\label{sec:high-school-walkthrough}
\subsection*{\textbf{Setup and procedure}}
The second evaluation session was conducted in a spacious conference room.
A large children's floor mat was added to define a playful zone, following recommendations from the first experiment.
The robot used the updated Version 2 of the dashboard interface, which incorporated changes such as feedback intensity levels and different LED color options.

The session involved four high school students, divided into two pairs.
Each pair was asked to role-play as younger children (elementary or middle school age) participating in a turn-taking Lego activity.
The robot was operated by an evaluator from a nearby desk.
Before the session began, the robot was introduced to the participants to reduce initial distraction, another refinement based on the previous walkthrough.

Each pair of students participated in two stages:
\begin{itemize}
    \item \textbf{Step 1: Robot as Regulator} - The two "children" were asked to collaborate on building a Lego tower while occasionally engaging in simulated turn-taking violations.
          The robot was expected to intervene using visual and movement-based feedback.
    \item \textbf{Step 2: Child as Regulator} - One student built a Lego structure with help from the robot, while the second student acted as a mediator.
          The robot would occasionally behave incorrectly (e.g., bringing the wrong Lego piece or running away), prompting the mediator to "teach" the robot how to behave, following the Learning by Teaching paradigm.
    \item \textbf{Step 3: Co-Design Discussion} - After the interaction tasks, students were invited to openly discuss their experience with the robot, suggest new behaviors, and reflect on what they would expect from a social robot in this context.
          The session was semi-structured and aimed at eliciting child-led ideas to inform future iterations.
\end{itemize}

The same three-stage procedure was repeated with the second pair of students.
During the first round, all four students were present and participated in observing or acting.
However, for the second round, only the second pair remained, as the first group had to leave the session early due to other commitments.

\subsection*{\textbf{Key observations}}
\begin{itemize}
    \item Red LED signals alone were insufficient to attract attention, especially due to ambient lighting and the robot's position on the floor.
    \item The lack of sound in negative feedback led to the robot being ignored during turn-taking violations. Participants were focused on the Lego task and did not notice subtle visual cues.
    \item A more effective attention-grabbing strategy was to have the robot "invade" the children's personal space with a quick approach motion.
    \item The robot being named by the students helped increase its perceived presence and relational value during interactions.
    \item One participant suggested that the robot could turn around and "walk away" when reacting negatively, simulating being upset or "offended."
\end{itemize}

\subsection*{\textbf{Design implications}}
Observations from this walkthrough demonstrated that visual cues alone were often insufficient to capture attention, particularly when the robot was positioned on the floor.
To address this, movement-based feedback was amplified, with more pronounced and even slightly intrusive motions, such as quick approaches or turning away, used to reinforce the robot's presence and personality.
The practice of letting children name the robot was recognized as a valuable means of enhancing engagement and relational dynamics, especially in scenarios involving repeated or long-term interaction.
These findings validated the flexible and expressive behavior options introduced after the expert walkthrough, and led to further interface refinements, such as the addition of more LED color choices to support clearer signaling during tasks that required specific requests.
The overall design thus became more attuned to real-world interaction needs and child-driven engagement.

\subsection{\textbf{Walkthrough with Middle School Students}}\label{sec:middle-school-walkthrough}
\subsection*{\textbf{Setup and procedure}}
The final evaluation session was conducted in the same spacious conference room used in the previous walkthrough.
A children's floor mat was again placed at the center of the space, but this time smaller Lego bricks were provided to increase task flexibility and challenge.
The robot was operated using the final version of the dashboard interface.
Three girls who had recently started middle school participated in the session.
They were told beforehand that the experiment was a Wizard-of-Oz setup, and the robot was introduced at the beginning of the session to help mitigate distractions.

The team consisted of the operator, an observing researcher, and an additional facilitator who sat on the floor with the participants, guiding them through each experimental step and prompting them for feedback.
The session followed the same structure as previous walkthroughs but was run with all three children at the same time.

\begin{itemize}
    \item \textbf{Step 1: Robot as Regulator} - The children were asked to collaboratively build a Lego tower while occasionally breaking the turn-taking rule.
          The robot attempted to intervene using lights, movement, and audio cues.
          However, the feedback signals were often ignored or went unnoticed due to high ambient noise and the robot's position on the floor.
    \item \textbf{Step 2: Child as Regulator} - Two children played with the robot while the third acted as a regulator.
          The robot sometimes performed incorrect or disruptive actions (e.g., stealing pieces or pretending to help but failing), prompting the regulator to guide or correct the behavior.
          The robot responded with positive or negative signals based on this guidance.
    \item \textbf{Step 3: Social Behavior and Emotion Teaching} - In a final interaction, children were asked to teach the robot about concepts like personal space, polite requesting, and emotional understanding.
          The robot performed incorrect distances or behaviors, prompting the children to explain or demonstrate appropriate actions.
\end{itemize}

\subsection*{\textbf{Key observations}}
\begin{itemize}
    \item The robot continued to be perceived more as a pet (e.g., a dog) than as a peer or authority figure, reinforcing previous observations.
    \item Children named the robot and began interacting with it socially, including petting it on the box/head and decorating it with Lego bricks.
    \item Visual feedback like LEDs remained mostly ineffective; participants suggested that a table-based setup could raise the robot to eye level and improve visibility.
    \item Fast and unpredictable movements were effective at capturing attention, sometimes even surprising the participants.
    \item Positive feedback involving lights, sound, and movement ("dances") was appreciated and engaging.
    \item Participants showed strong interest in how the robot could recognize human emotions and tailor its behavior accordingly.
    \item During Step 2, children experimented with rewarding the robot for good behavior, and teaching it when to stop disruptive actions.
    \item In Step 3, they proposed using facial expressions, tone of voice, and environmental context to assess emotions and social cues.
    \item They also expressed a desire for the robot to have an onboard interface to communicate or show its own internal emotional state.
    \item After extended time with the robot (over one hour), the children continued to engage with it playfully while discussing social and emotional aspects.
\end{itemize}

\subsection*{\textbf{Design implications}}
The findings from this session highlighted the importance of acknowledging the robot's perceived identity, as children consistently related to it more as a pet-like companion than as a peer or authority figure.
This insight informed the ongoing design by encouraging the use of playful, multimodal feedback, combining lights, sound, and expressive movement, with the intensity of cues scaled to the importance of the situation.
Suggestions to improve the visibility of visual signals, such as elevating the robot or adding auxiliary displays, were considered to address the limitations of floor-based interactions.
Furthermore, the children's enthusiasm for emotional awareness features, like interpreting tone of voice, facial expressions, and proximity, pointed toward future enhancements in autonomous behavior and social context sensitivity.
The idea of equipping the robot with an on-device interface also emerged as a promising direction for enabling more direct, child-driven communication and mutual understanding.
Overall, co-design insights from this session reinforced the value of tailoring robot behavior to both the social context and the emotional landscape of the interaction.

\subsection{\textbf{Summary of Iterative Design Changes}}
This section summarizes the key iterative changes that shaped both the dashboard interface and the robot's behavior across the three evaluation sessions.
Each walkthrough revealed new user needs and inspired concrete refinements.
Interface updates were driven by usability and expressiveness goals, while behavioral adaptations emerged through co-design discussions and observational insights.
Screenshots of the evolving interface are included to illustrate the progression from the initial prototype to the final deployed version.

\subsubsection*{\textbf{Version 1 - Initial Prototype}}
\begin{itemize}
    \item Basic joystick and pre-defined movement controls.
    \item Minimal bad-behavior feedback capabilities (red LED, spin, sound).
    \item No dedicated gripper control (box could only be opened/closed by moving the arm); no feedback intensity levels.
\end{itemize}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/dashboard_v1.png}
    \caption{Version 1 - Initial prototype with limited robot control and feedback options.}
    \label{fig:dashboard-v1}
\end{figure}

\subsubsection*{\textbf{Version 2 - Post Expert Walkthrough}}
\begin{itemize}
    \item Added gripper controls and advanced LED configuration (colors, blink timing, speed).
    \item Introduced feedback modes: positive and negative with scalable intensity.
    \item Refined movement control and added panic button.
    \item Macro-scenarios for quick intervention.
    \item UI layout was reorganized into functional panels or "zones" to streamline operator access to robot features.
\end{itemize}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/dashboard_v2.png}
    \caption{Version 2 - Refined interface after expert feedback, supporting greater flexibility.}
    \label{fig:dashboard-v2}
\end{figure}

\subsubsection*{\textbf{Version 3 - Current Version for Pilot Study}}
\begin{itemize}
    \item Further expanded LED signaling options for color-specific communication.
    \item Audio feedback support for positive cues only.
    \item Increased the size and visual prominence of the panic button to improve operator responsiveness during high-pressure scenarios.
\end{itemize}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/dashboard_v3.png}
    \caption{Version 3 - Final dashboard interface used in the pilot experiment.}
    \label{fig:dashboard-v3}
\end{figure}

\subsection{\textbf{Behavioral Design Iterations}}

Beyond interface adjustments, each walkthrough also shaped our understanding of effective robot behaviors in child-facing settings.

Across the three walkthroughs, a consistent pattern emerged in how children and experts responded to the robot's behavior.
Rather than detailing new robot actions, participants influenced the style, tone, and social framing of these actions, shifting the robot from an authority figure to a pet-like peer.
This shift affected the design of feedback modalities (e.g., neutral negative feedback, celebratory positive feedback) and the narrative framing of the robot within the experiment.
These reflections did not result in autonomous learning, but they directly shaped the robot's available actions and responses,
expanding its behavioral repertoire and enabling the operator to deliver more socially aligned interventions.
This iterative refinement represented one of the three core directions explored throughout the experiments and laid the groundwork for Step 3 of the study,
where the robot begins to apply these socially informed behaviors in response to interaction cues.
While still operator-driven, this stage anticipates future developments toward semi-autonomous behavior grounded in prior human-led interventions (see Section~\ref{sec:design}).
\\
Key insights included:
\begin{itemize}
    \item \textbf{Framing of the Robot:} Children consistently perceived the robot as a playful companion or pet, rather than an authority figure.\\
          This influenced design decisions toward peer-like, non-disciplinary behaviors. [Exp 1--3]
    \item \textbf{Feedback Intensity:} Both experts and children emphasized the importance of scalable, multimodal feedback (e.g., lights + movement + sound), especially for positive interactions.\\
          Negative feedback should remain neutral and minimal to avoid encouraging misbehavior. [Exp 1--3]
    \item \textbf{Social Engagement:} Naming the robot, petting it, and attributing personality to it emerged naturally, suggesting personalization features may enhance engagement. [Exp 2--3]
    \item \textbf{Learning by Teaching:} Children actively guided the robot using verbal and physical cues, validating the LbT framing.\\
          Tasks like teaching the robot to maintain distance or follow polite commands proved intuitive and effective. [Exp 2--3]
    \item \textbf{Emotion and Context Awareness:} Middle school students suggested that robots should use emotional cues, facial expressions, and context to decide how and when to interact. [Exp 3]
\end{itemize}

These behavioral reflections informed the structure of robot actions available through the dashboard, highlighting design directions for future exploration of semi-autonomous and emotionally aware systems.

\newpage

\section{\textbf{Conclusions}}\label{sec:conclusions}
This thesis explored how the Learning by Teaching (LbT) paradigm can be instantiated in child-robot interaction using a Wizard-of-Oz setup.
Positioned within the broader TESORO project, the work focused on both technical development and exploratory research.
A web-based dashboard was designed and implemented to control a RoboMaster EP robot in a collaborative Lego-building task, enabling real-time interventions through multimodal feedback.

Through three walkthrough studies involving HCI experts, high school students, and middle school children, we iteratively refined both the dashboard interface and the robot's behavior.
These sessions provided key insights into how children perceive and interact with social robots, especially when framed as teachable peers.
The evaluation highlighted the importance of personalization, emotional attunement, and multimodal expressiveness in shaping engagement and understanding.

Rather than aiming to measure fixed learning outcomes, this work pursued an exploratory path, surfacing the nuances of teaching dynamics, role framing, and feedback preferences.
The findings demonstrate how interface co-design and behavioral framing can work in tandem to scaffold rich, socially grounded interaction between children and robots.

The result is a dual contribution: a robust control interface for socially expressive robot operation, and a set of exploratory insights into child-led teaching dynamics.
Together, they lay the foundation for future systems where children can not only be supported by social robots but also shape and teach their behavior in real time.

\subsection{\textbf{Future Work}}
Future directions include extending the current Wizard-of-Oz setup toward partial autonomy, where the robot learns behavioral rules from child input across sessions.
Integrating emotion recognition, contextual awareness, and personalization mechanisms would enable the robot to better align with the child's intentions and needs.

Another promising direction involves expanding co-design with a broader range of users, particularly children with cognitive or communicative disabilities, in line with TESORO's inclusive vision.
Long-term studies will also be necessary to assess whether repeated teaching interactions lead to meaningful changes in children's social skills, sense of agency, or emotional understanding.

Technically, extending the dashboard to support on-device interfaces, real-time analytics, or adaptive feedback based on multimodal sensing could further bridge the gap between controlled experimentation and autonomous, socially intelligent behavior.

Ultimately, this thesis aims to serve as a stepping stone toward socially attuned robots that are not only tools for teaching but also active participants in co-constructed learning experiences with children.

\newpage

%%%%% BIBLIOGRAPHY %%%%%
\bibliographystyle{abbrv}
\bibliography{references}

\end{document}
